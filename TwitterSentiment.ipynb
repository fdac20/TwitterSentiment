{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# University Related Coronavirus Sentiment Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insert generic writeup here**\n",
    "\n",
    "We are using the NLTK package in Python to do our natural language processing tasks in this project. Let's start with some basic setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data for Training and Testing our Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command downloads a data set contained within NLTK. This is a collection of 20,000 tweets which will be used to test and train our model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This imports 3 datasets contained within the \"twitter_samples\" folder which we downloaded earlier.\n",
    "\n",
    "```negative_tweets.json```: A collection of 5,000 tweets with negative sentiment <br/>\n",
    "```positive_tweets.json```: A collection of 5,000 tweets with positive sentiment <br/>\n",
    "```tweets.20150430-223406.json```: A collection of 10,000 tweets with no sentiment label\n",
    "\n",
    "The 10,000 positive and negative tweets will be used to train our model, and the remmaining 10,000 will be used to test it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are numerous ways we can \"clean\" our data to make our final model better. First, we will do what is called \"tokenizing.\"\n",
    "This process will take the Tweets as a whole, and split it into smaller subsections called tokens. These tokens make it much\n",
    "easier for machines to understand the context of the text when developing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by storing the positive, negative, and general tweets as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "nonpolar = twitter_samples.strings('tweets.20150430-223406.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, NLTK contains another helpful resource known as ```punkt```. This is a pre-trained model that allows us to easily tokenize our data.\n",
    "\n",
    "To get the ```punkt``` resrouce, we run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are able to utilize NLTK's powerful tokenization tools. We simply use the ```.tokenized()``` method in order to tokenize our data.\n",
    "\n",
    "To demonstrate how this works, let's tokenize ```negative_tweets.json```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hopeless for tmr :(\n",
      "['hopeless', 'for', 'tmr', ':(']\n"
     ]
    }
   ],
   "source": [
    "print(twitter_samples.strings('negative_tweets.json')[0])   # String\n",
    "print(twitter_samples.tokenized('negative_tweets.json')[0]) # The same string, tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and tokenize ```positive_tweets.json``` for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization, in terms of natural language processing, is the process of transforming a text into a canonical (standard) form.\n",
    "For example, \"gooood,\" and \"gud\" can be resolved to the normalized form \"good.\" This can also apply for different tenses of the same word. For example, \"ran,\" \"runs,\" and \"running\" are all forms of \"run.\"\n",
    "\n",
    "<br/>\n",
    "\n",
    "#### There are a few things at work here:\n",
    "\n",
    "Stemming is the process of removing suffixes and prefixes from words. As an example, it reduces the inflection in words such as \"troubled\" and \"troubles\" to their root form \"trouble.\"\n",
    "\n",
    "Here are some stemming examples made using Porters Algorithm, one of the most common stemming algorithms:\n",
    "\n",
    "<html>\n",
    "<img src=\"Documents/StemmingExample.PNG\" alt=\"drawing\" width=\"275\"/>\n",
    "</html>\n",
    "\n",
    "Lemmatization is similar to stemming, but rather than just cutting off the affixes, it will transform the word to it's root. As an example, it may transform the word \"better\" to \"good.\"\n",
    "\n",
    "Here are some examples of lemmatization using a dictionary mapping for the translations:\n",
    "\n",
    "<html>\n",
    "<img src=\"Documents/LemmatizationExample.PNG\" alt=\"drawing\" width=\"275\"/>\n",
    "</html>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "This processing is essential for noisy social-media posts, as abbreviations and mispellings are very common!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using lemmatization for our data, so let's download ```wordnet```, a lexical database, and ```averaged_perceptron_tagger```, which will help us in determining context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the lemmatizer, we must determine the context of each word within our tweets. To do this, we use what's called a tagging algorithm. Fortunately, NLTK provides a function for this.\n",
    "\n",
    "Let's test it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "print(pos_tag(tweet_tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some common tags and their meaning:\n",
    "- NNP: Noun, proper, singular\n",
    "- NN: Noun, common, singular or mass\n",
    "- IN: Preposition or conjunction, subordinating\n",
    "- VBG: Verb, gerund or present participle\n",
    "- VBN: Verb, past participle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the fact that tags starting with ``NN`` are typically nouns, and tags starting with ```VB``` are typically verbs, we can incorporate this into a function to lemmatize our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function gets the tag of each token within the Tweet, and lemmatizes accordingly.\n",
    "Let's test it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'be', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatize_sentence(tweet_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
