{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# University Related Coronavirus Sentiment Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insert generic writeup here**\n",
    "\n",
    "We are using the NLTK package in Python to do our natural language processing tasks in this project. Let's start with some basic setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, there are many ways we can help the computer process our data. There are numerous ways which we can \"clean\" our data to make our final prediction more accurate. Let's get into some of these methods below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting our Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have collected our data in csv format, so let's put it in a Pandas dataframe and pull out the Tweets and place them in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "# Extract content and date fields from csv\n",
    "fields=['date', 'content']\n",
    "d = pd.read_csv('combined_csv.csv', usecols=fields)\n",
    "\n",
    "uni_related_tweets = d['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will do what is called \"tokenizing.\" This process will take the Tweets as a whole, and split it into smaller subsections called tokens. These tokens make it much easier for machines to understand the context of the text when developing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate how tokenization works, lets tokenize some Tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marcus Yoder (HCB ‘92) has been appointed senior vice president of sales for @GameAccountNtwk, a leading business-to-business supplier of internet gambling software-as-a-service solutions. https://t.co/a8uu63502V https://t.co/WqS4p0PVN9\n",
      "\n",
      "['Marcus', 'Yoder', '(', 'HCB', '‘', '92', ')', 'has', 'been', 'appointed', 'senior', 'vice', 'president', 'of', 'sales', 'for', '@', 'GameAccountNtwk', ',', 'a', 'leading', 'business-to-business', 'supplier', 'of', 'internet', 'gambling', 'software-as-a-service', 'solutions', '.', 'https', ':', '//t.co/a8uu63502V', 'https', ':', '//t.co/WqS4p0PVN9']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(uni_related_tweets[0])   # String\n",
    "print()\n",
    "print(word_tokenize(uni_related_tweets[0])) # The same string, tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and tokenize all of these tweets, and store them for use later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [Marcus, Yoder, (, HCB, ‘, 92, ), has, been, a...\n",
       "1       [Congratulations, to, the, Haslam, College, of...\n",
       "2       [#, FacultyFriday, Supply, chain, management, ...\n",
       "3       [Due, to, COVID-19, ,, most, internships, have...\n",
       "4       [Congratulations, to, MSCM, Tri-Con, grad, Nai...\n",
       "                              ...                        \n",
       "7038    [2, weeks, from, tonight, is, Torch, Night, .,...\n",
       "7039    [This, is, great, news, Tom, ., Thanks, for, t...\n",
       "7040    [Having, a, partner, who, is, the, love, of, y...\n",
       "7041    [So, impressed, with, the, crowd, &, amp, ;, t...\n",
       "7042    [More, good, news, about, UTK, from, US, News,...\n",
       "Name: content, Length: 7043, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, sentence in enumerate(uni_related_tweets):\n",
    "    uni_related_tweets[i] = word_tokenize(uni_related_tweets[i])\n",
    "uni_related_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the tokenization process took the original string and split it into smaller subsections known as tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization, in terms of natural language processing, is the process of transforming a text into a canonical (standard) form.\n",
    "For example, \"gooood,\" and \"gud\" can be resolved to the normalized form \"good.\" This can also apply for different tenses of the same word. For example, \"ran,\" \"runs,\" and \"running\" are all forms of \"run.\"\n",
    "\n",
    "<br/>\n",
    "\n",
    "#### There are a few things at work here:\n",
    "\n",
    "Stemming is the process of removing suffixes and prefixes from words. As an example, it reduces the inflection in words such as \"troubled\" and \"troubles\" to their root form \"trouble.\"\n",
    "\n",
    "Here are some stemming examples made using Porters Algorithm, one of the most common stemming algorithms:\n",
    "\n",
    "<html>\n",
    "<img src=\"Documents/StemmingExample.PNG\" alt=\"drawing\" width=\"275\"/>\n",
    "</html>\n",
    "\n",
    "Lemmatization is similar to stemming, but rather than just cutting off the affixes, it will transform the word to it's root. As an example, it may transform the word \"better\" to \"good.\"\n",
    "\n",
    "Here are some examples of lemmatization using a dictionary mapping for the translations:\n",
    "\n",
    "<html>\n",
    "<img src=\"Documents/LemmatizationExample.PNG\" alt=\"drawing\" width=\"275\"/>\n",
    "</html>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "This processing is essential for noisy social-media posts, as abbreviations and mispellings are very common!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using lemmatization for our data, so let's download ```wordnet```, a lexical database, and ```averaged_perceptron_tagger```, which will help us in determining context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the lemmatizer, we must determine the context of each word within our tweets. To do this, we use what's called a tagging algorithm. Fortunately, NLTK provides a function for this.\n",
    "\n",
    "Let's test it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "print(pos_tag(tweet_tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some common tags and their meaning:\n",
    "- NNP: Noun, proper, singular\n",
    "- NN: Noun, common, singular or mass\n",
    "- IN: Preposition or conjunction, subordinating\n",
    "- VBG: Verb, gerund or present participle\n",
    "- VBN: Verb, past participle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the fact that tags starting with ``NN`` are typically nouns, and tags starting with ```VB``` are typically verbs, we can incorporate this into a function to lemmatize our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function gets the tag of each token within the Tweet, and lemmatizes accordingly.\n",
    "Let's test it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'be', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatize_sentence(tweet_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
