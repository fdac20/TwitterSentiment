{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# University Related Coronavirus Sentiment Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insert generic writeup here**\n",
    "\n",
    "We are using the NLTK package in Python to do our natural language processing tasks in this project. Let's start with some basic setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are numerous ways we can \"clean\" our data to make our final predicitions better. First, we will do what is called \"tokenizing.\"\n",
    "This process will take the Tweets as a whole, and split it into smaller subsections called tokens. These tokens make it much\n",
    "easier for machines to understand the context of the text when developing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/tucker/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# # Extract content and date fields from csv\n",
    "# fields=['date', 'content']\n",
    "# d = pd.read_csv('combined_csv.csv', usecols=fields)\n",
    "\n",
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, NLTK contains another helpful resource known as ```punkt```. This is a pre-trained model that allows us to easily tokenize our data.\n",
    "\n",
    "To get the ```punkt``` resrouce, we run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tucker/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are able to utilize NLTK's powerful tokenization tools. We simply use the ```.tokenized()``` method in order to tokenize our data.\n",
    "\n",
    "To demonstrate how this works, let's tokenize ```negative_tweets.json```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Tokenized:\n",
      "hopeless for tmr :(\n",
      "\n",
      "Tokenized:\n",
      "['hopeless', 'for', 'tmr', ':(']\n"
     ]
    }
   ],
   "source": [
    "print('Not Tokenized:')\n",
    "print(twitter_samples.strings('negative_tweets.json')[0])   # String\n",
    "print()\n",
    "print('Tokenized:')\n",
    "print(twitter_samples.tokenized('negative_tweets.json')[0]) # The same string, tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and tokenize ```positive_tweets.json``` for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "print(tweet_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization, in terms of natural language processing, is the process of transforming a text into a canonical (standard) form.\n",
    "For example, \"gooood,\" and \"gud\" can be resolved to the normalized form \"good.\" This can also apply for different tenses of the same word. For example, \"ran,\" \"runs,\" and \"running\" are all forms of \"run.\"\n",
    "\n",
    "<br/>\n",
    "\n",
    "#### There are a few things at work here:\n",
    "\n",
    "Stemming is the process of removing suffixes and prefixes from words. As an example, it reduces the inflection in words such as \"troubled\" and \"troubles\" to their root form \"trouble.\"\n",
    "\n",
    "Here are some stemming examples made using Porters Algorithm, one of the most common stemming algorithms:\n",
    "\n",
    "<html>\n",
    "<img src=\"Documents/StemmingExample.PNG\" alt=\"drawing\" width=\"275\"/>\n",
    "</html>\n",
    "\n",
    "Lemmatization is similar to stemming, but rather than just cutting off the affixes, it will transform the word to it's root. As an example, it may transform the word \"better\" to \"good.\"\n",
    "\n",
    "Here are some examples of lemmatization using a dictionary mapping for the translations:\n",
    "\n",
    "<html>\n",
    "<img src=\"Documents/LemmatizationExample.PNG\" alt=\"drawing\" width=\"275\"/>\n",
    "</html>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "This processing is essential for noisy social-media posts, as abbreviations and mispellings are very common!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using lemmatization for our data, so let's download ```wordnet```, a lexical database, and ```averaged_perceptron_tagger```, which will help us in determining context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/tucker/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/tucker/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the lemmatizer, we must determine the context of each word within our tweets. To do this, we use what's called a tagging algorithm. Fortunately, NLTK provides a function for this.\n",
    "\n",
    "Let's test it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "print(pos_tag(tweet_tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some common tags and their meaning:\n",
    "- NNP: Noun, proper, singular\n",
    "- NN: Noun, common, singular or mass\n",
    "- IN: Preposition or conjunction, subordinating\n",
    "- VBG: Verb, gerund or present participle\n",
    "- VBN: Verb, past participle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the fact that tags starting with ``NN`` are typically nouns, and tags starting with ```VB``` are typically verbs, we can incorporate this into a function to lemmatize our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function gets the position tag of each token in the tweet, and lemmatizes accordingly.\n",
    "Let's test it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'be', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatize_sentence(tweet_tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Noise from the Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of noise in tweets. Things such as hyperlinks, twitter handles, and some punctuation must be removed. That is what we will do here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove hyperlinks, we'll search for substrings ```http://``` and ```https://``` followed by the rest of the address. Once found, we will replace with an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function removes noise and also incorporates the normalization and lemmatization as seen earlier. As arguments, it takes Tweet tokens and a stop words tuple.\n",
    "\n",
    "Once again, NLTK is of great help here, as it provides a great stopwords set for us to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tucker/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's briefly test this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n",
      "\n",
      "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(tweet_tokens[0])\n",
    "print()\n",
    "print(remove_noise(tweet_tokens[0], stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we've removed all mentions, removed all stop words, and have made everything lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's go ahead and use this function to clean up the Tweets we saved earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in positive_tweet_tokens:\n",
    "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "for tokens in negative_tweet_tokens:\n",
    "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n",
      "\n",
      "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(positive_tweet_tokens[0])\n",
    "print()\n",
    "print(positive_cleaned_tokens_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much, much better! All of this processing will allow our model to be much more accurate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Density\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the simplest forms of textual analysis is a simple word frequency. Let's take a look at the frequencies of words in some of these data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "all_neg_words = get_all_words(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a generator function which will take a list of tokens as an argument and will provide a list of all of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've combined all of the positive tweets, let's see what the most popular ones are. Once again, NLTK comes to the rescue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253), ('u', 245), ('day', 242), ('like', 229), ('see', 195), ('happy', 192)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(freq_dist_pos.most_common(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for the negatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':(', 4585), (':-(', 501), (\"i'm\", 343), ('...', 332), ('get', 325), ('miss', 291), ('go', 275), ('please', 275), ('want', 246), ('like', 218), ('♛', 210), ('》', 210), ('u', 193), (\"can't\", 180), ('time', 160)]\n"
     ]
    }
   ],
   "source": [
    "freq_dist_neg = FreqDist(all_neg_words)\n",
    "print(freq_dist_neg.most_common(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. It's clear that emoticons are very common in both positive and negative Tweets. It's very interesting how you can begin to see some overall sentiment just from this frequency analysis, with the positive Tweets commonly having words such as \"thanks,\" \"love, \" and \"happy.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to summarize a bit, so far we have extracted tweets from NLTK, tokenized them, normalized them, and cleaned them up. Then, we looked at some basic frequency analysis. Let's move on to modeling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping for Model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a model, we need to create a training set to train our model. This is known as supervised learning, the task of learning a function that maps an input to an output based on example input-output pairs. We must associate our datasets with a sentiment, and these is where our datasets of positive and negative datasets comes in. Here, we will train our model to classify into two categories, positive and negative. In order to do this, we will need to split our dataset into parts, one part for training the model and the another part for testing its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the Naive Bayes Classifier in NLTK. If you've taken a statistics class, you've probably heard of Bayes' Thereom, and that is what this classifier is based on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the Naive Bayes Classfier in NLTK, we must ocnvert our tokens to a dictionary with the words being the keys and the value being True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a generator function which will convert the clean tokens to dictionaries with the words being the keys and the values all being True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, we have to split our dataset. Part of it will be used for training, and the other for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This attaches a sentiment to each tweet, and then combines them. To avoid bias, the combines dataset is shuffled.\n",
    "\n",
    "After the above code runs, we have 10,000 Tweets to build our model. 7,000 will be used for training while the remaining 3,000 will be used for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all of this setup, we are finally to the point where we will build our model. We'll use some handy NLTK functions here: ```train()``` and ```accuracy```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Testing our Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9976666666666667\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2072.6 : 1.0\n",
      "                      :) = True           Positi : Negati =   1638.4 : 1.0\n",
      "                follower = True           Positi : Negati =     38.2 : 1.0\n",
      "                     sad = True           Negati : Positi =     23.1 : 1.0\n",
      "                     bam = True           Positi : Negati =     19.6 : 1.0\n",
      "                 welcome = True           Positi : Negati =     18.5 : 1.0\n",
      "                     x15 = True           Negati : Positi =     17.1 : 1.0\n",
      "              appreciate = True           Positi : Negati =     14.9 : 1.0\n",
      "                    glad = True           Positi : Negati =     14.1 : 1.0\n",
      "                followed = True           Negati : Positi =     13.9 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy which was outputted above is the percentage of tweets that the model was correct. 99.7%, not too bad!\n",
    "\n",
    "We also outputted the most informative features. This shows the ratio of that specific tokens occurence in the positive dataset to the negative dataset. Some more interesting insight into our data here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are to the point that we can pass our classifier a Tweet, and it will return it's sentiment. Let's Test it out!\n",
    "\n",
    "We'll write our own example for a postive and negative tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "negative_tweet = 'Today was a terrible day! I spent forever trying to get my Jupyter Notebook to run properly, but it wouldn\\'t budge! :('\n",
    "positive_tweet = 'Today was amazing! I\\'m very happy! :)'\n",
    "                                                                                                                                        \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = remove_noise(word_tokenize(negative_tweet))\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in tokens)))\n",
    "                                                                                                                                        \n",
    "tokens = remove_noise(word_tokenize(positive_tweet))\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working as intended!\n",
    "\n",
    "With our model set up and working, let's move on to using it for something cool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in and Classifying our Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, we are comparing two datasets. One dataset containing Tweets from accounts which are directly associated or owned by the University, and the other containing Tweets which are related to the university, but not directly associated or owned by the university.\n",
    "\n",
    "For the tweets from accounts which are directly associated or owned by the University, we searched for Tweets from these accounts:\n",
    "\n",
    "Administration:\n",
    "- @DondePlowman - Donde Plowman\n",
    "- @randyboyd - Randy Boyd\n",
    "- @tucarpenter - Tiffany Carpenter\n",
    "- @UTIA_SVP - Tim Cross\n",
    "- @KC4UTM - Keith Carver\n",
    "\n",
    "University Accounts:\n",
    "- @utknoxville\n",
    "- @utk_tce\n",
    "- @utkdos\n",
    "- @ut_admissions\n",
    "- @utk_asc\n",
    "- @UTKCEHHS\n",
    "- @utk_cfs\n",
    "- @UTKStudentLife\n",
    "- @UTKCoAD\n",
    "- @UTKSOM\n",
    "- @tennalum\n",
    "- @utknursing\n",
    "- @HaslamUT\n",
    "\n",
    "For the tweets from accounts which are not directly associated or owned by the University, we searched for Tweets with these keywords\n",
    "- keyword1\n",
    "- keyword2\n",
    "- keyword3...\n",
    "\n",
    "This data was colleced using the snscrape command line tool, and historically represent the time period of August 1st, 2019 to August 1st, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we, have our data in CSVs, let's use Pandas to read them in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>user/username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-08-10T14:13:27+00:00</td>\n",
       "      <td>Marcus Yoder (HCB ‘92) has been appointed seni...</td>\n",
       "      <td>HaslamUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-08-07T19:30:08+00:00</td>\n",
       "      <td>Congratulations to the Haslam College of Busin...</td>\n",
       "      <td>HaslamUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-08-07T14:14:23+00:00</td>\n",
       "      <td>#FacultyFriday Supply chain management profess...</td>\n",
       "      <td>HaslamUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-08-06T19:37:43+00:00</td>\n",
       "      <td>Due to COVID-19, most internships have switche...</td>\n",
       "      <td>HaslamUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-08-06T14:40:37+00:00</td>\n",
       "      <td>Congratulations to MSCM Tri-Con grad Nainika S...</td>\n",
       "      <td>HaslamUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7038</th>\n",
       "      <td>2019-08-04T11:55:30+00:00</td>\n",
       "      <td>2 weeks from tonight is Torch Night. “One that...</td>\n",
       "      <td>DondePlowman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7039</th>\n",
       "      <td>2019-08-03T17:07:33+00:00</td>\n",
       "      <td>This is great news Tom. Thanks for the update....</td>\n",
       "      <td>DondePlowman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7040</th>\n",
       "      <td>2019-08-02T22:56:37+00:00</td>\n",
       "      <td>Having a partner who is the love of your life ...</td>\n",
       "      <td>DondePlowman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7041</th>\n",
       "      <td>2019-08-02T16:39:28+00:00</td>\n",
       "      <td>So impressed with the crowd &amp;amp; the passion ...</td>\n",
       "      <td>DondePlowman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7042</th>\n",
       "      <td>2019-08-02T10:30:34+00:00</td>\n",
       "      <td>More good news about UTK from US News rankings...</td>\n",
       "      <td>DondePlowman</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7043 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           date  \\\n",
       "0     2020-08-10T14:13:27+00:00   \n",
       "1     2020-08-07T19:30:08+00:00   \n",
       "2     2020-08-07T14:14:23+00:00   \n",
       "3     2020-08-06T19:37:43+00:00   \n",
       "4     2020-08-06T14:40:37+00:00   \n",
       "...                         ...   \n",
       "7038  2019-08-04T11:55:30+00:00   \n",
       "7039  2019-08-03T17:07:33+00:00   \n",
       "7040  2019-08-02T22:56:37+00:00   \n",
       "7041  2019-08-02T16:39:28+00:00   \n",
       "7042  2019-08-02T10:30:34+00:00   \n",
       "\n",
       "                                                content user/username  \n",
       "0     Marcus Yoder (HCB ‘92) has been appointed seni...      HaslamUT  \n",
       "1     Congratulations to the Haslam College of Busin...      HaslamUT  \n",
       "2     #FacultyFriday Supply chain management profess...      HaslamUT  \n",
       "3     Due to COVID-19, most internships have switche...      HaslamUT  \n",
       "4     Congratulations to MSCM Tri-Con grad Nainika S...      HaslamUT  \n",
       "...                                                 ...           ...  \n",
       "7038  2 weeks from tonight is Torch Night. “One that...  DondePlowman  \n",
       "7039  This is great news Tom. Thanks for the update....  DondePlowman  \n",
       "7040  Having a partner who is the love of your life ...  DondePlowman  \n",
       "7041  So impressed with the crowd &amp; the passion ...  DondePlowman  \n",
       "7042  More good news about UTK from US News rankings...  DondePlowman  \n",
       "\n",
       "[7043 rows x 3 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Extract content and date fields from csv\n",
    "fields=['date', 'content', 'user/username']\n",
    "d = pd.read_csv('combined_csv.csv', usecols=fields)\n",
    "\n",
    "# Create dataframe\n",
    "tweets_df = pd.DataFrame(data=d)\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our model and add a column to our dataframe which represents the sentiment of each Tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>user/username</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-08-10T14:13:27+00:00</td>\n",
       "      <td>Marcus Yoder (HCB ‘92) has been appointed seni...</td>\n",
       "      <td>HaslamUT</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-08-07T19:30:08+00:00</td>\n",
       "      <td>Congratulations to the Haslam College of Busin...</td>\n",
       "      <td>HaslamUT</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-08-07T14:14:23+00:00</td>\n",
       "      <td>#FacultyFriday Supply chain management profess...</td>\n",
       "      <td>HaslamUT</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-08-06T19:37:43+00:00</td>\n",
       "      <td>Due to COVID-19, most internships have switche...</td>\n",
       "      <td>HaslamUT</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-08-06T14:40:37+00:00</td>\n",
       "      <td>Congratulations to MSCM Tri-Con grad Nainika S...</td>\n",
       "      <td>HaslamUT</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7038</th>\n",
       "      <td>2019-08-04T11:55:30+00:00</td>\n",
       "      <td>2 weeks from tonight is Torch Night. “One that...</td>\n",
       "      <td>DondePlowman</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7039</th>\n",
       "      <td>2019-08-03T17:07:33+00:00</td>\n",
       "      <td>This is great news Tom. Thanks for the update....</td>\n",
       "      <td>DondePlowman</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7040</th>\n",
       "      <td>2019-08-02T22:56:37+00:00</td>\n",
       "      <td>Having a partner who is the love of your life ...</td>\n",
       "      <td>DondePlowman</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7041</th>\n",
       "      <td>2019-08-02T16:39:28+00:00</td>\n",
       "      <td>So impressed with the crowd &amp;amp; the passion ...</td>\n",
       "      <td>DondePlowman</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7042</th>\n",
       "      <td>2019-08-02T10:30:34+00:00</td>\n",
       "      <td>More good news about UTK from US News rankings...</td>\n",
       "      <td>DondePlowman</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7043 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           date  \\\n",
       "0     2020-08-10T14:13:27+00:00   \n",
       "1     2020-08-07T19:30:08+00:00   \n",
       "2     2020-08-07T14:14:23+00:00   \n",
       "3     2020-08-06T19:37:43+00:00   \n",
       "4     2020-08-06T14:40:37+00:00   \n",
       "...                         ...   \n",
       "7038  2019-08-04T11:55:30+00:00   \n",
       "7039  2019-08-03T17:07:33+00:00   \n",
       "7040  2019-08-02T22:56:37+00:00   \n",
       "7041  2019-08-02T16:39:28+00:00   \n",
       "7042  2019-08-02T10:30:34+00:00   \n",
       "\n",
       "                                                content user/username  \\\n",
       "0     Marcus Yoder (HCB ‘92) has been appointed seni...      HaslamUT   \n",
       "1     Congratulations to the Haslam College of Busin...      HaslamUT   \n",
       "2     #FacultyFriday Supply chain management profess...      HaslamUT   \n",
       "3     Due to COVID-19, most internships have switche...      HaslamUT   \n",
       "4     Congratulations to MSCM Tri-Con grad Nainika S...      HaslamUT   \n",
       "...                                                 ...           ...   \n",
       "7038  2 weeks from tonight is Torch Night. “One that...  DondePlowman   \n",
       "7039  This is great news Tom. Thanks for the update....  DondePlowman   \n",
       "7040  Having a partner who is the love of your life ...  DondePlowman   \n",
       "7041  So impressed with the crowd &amp; the passion ...  DondePlowman   \n",
       "7042  More good news about UTK from US News rankings...  DondePlowman   \n",
       "\n",
       "     sentiment  \n",
       "0     Negative  \n",
       "1     Positive  \n",
       "2     Positive  \n",
       "3     Positive  \n",
       "4     Positive  \n",
       "...        ...  \n",
       "7038  Positive  \n",
       "7039  Positive  \n",
       "7040  Positive  \n",
       "7041  Positive  \n",
       "7042  Positive  \n",
       "\n",
       "[7043 rows x 4 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment = []\n",
    "tweets = tweets_df['content'].tolist()\n",
    "for tweet in tweets:\n",
    "    tokens = remove_noise(word_tokenize(tweet))\n",
    "    sentiment.append(classifier.classify(dict([token, True] for token in tokens)))\n",
    "    \n",
    "tweets_df['sentiment'] = sentiment\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also take a quick look at the overall counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive    5595\n",
       "Negative    1448\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now we have associated a sentiment with each of these Tweets, as seen above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas doesn't like ISO 8601 time format, so let's use a lambda to change the dates to something readable by Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil.parser\n",
    "# Concert ISO 8601 time format to be readable by pandas\n",
    "tweets_df['date'] = tweets_df['date'].apply(lambda x: dateutil.parser.parse(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use groupby to get the number of positive and negative Tweets for each week in our time period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "month/year  sentiment\n",
       "0           Negative     119\n",
       "            Positive     441\n",
       "1           Negative     131\n",
       "            Positive     464\n",
       "2           Negative     124\n",
       "            Positive     489\n",
       "3           Negative     101\n",
       "            Positive     536\n",
       "4           Negative      99\n",
       "            Positive     321\n",
       "5           Negative     108\n",
       "            Positive     415\n",
       "6           Negative     110\n",
       "            Positive     392\n",
       "7           Negative     183\n",
       "            Positive     650\n",
       "8           Negative     167\n",
       "            Positive     536\n",
       "9           Negative     105\n",
       "            Positive     519\n",
       "10          Negative      92\n",
       "            Positive     363\n",
       "11          Negative      85\n",
       "            Positive     352\n",
       "12          Negative      24\n",
       "            Positive     117\n",
       "dtype: int64"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/27823273/counting-frequency-of-values-by-date-using-pandas\n",
    "tweets_df['month/year'] = tweets_df['date'].apply(lambda x: \"%d/%d\" % (x.month, x.year))\n",
    "months = ['8/2019', '9/2019', '10/2019', '11/2019', '12/2019', '1/2020', '2/2020', '3/2020', '4/2020', '5/2020', '6/2020', '7/2020', '8/2020']\n",
    "mapping = {day: i for i, day in enumerate(months)}\n",
    "tweets_df['month/year'] = tweets_df['month/year'].map(mapping)\n",
    "# tweets_df\n",
    "\n",
    "\n",
    "# tweets_df_grouped = tweets_df.groupby(['month/year', 'sentiment', 'map']).size()\n",
    "tweets_df_grouped = tweets_df.groupby(['month/year', 'sentiment']).size()\n",
    "tweets_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='month/year'>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df = tweets_df_grouped.to_frame()\n",
    "# df = df.sort_values(by=['map'])\n",
    "\n",
    "# tweets_df_grouped = tweets_df_grouped.sort_values(ascending=True)\n",
    "# tweets_df_grouped\n",
    "tweets_df_grouped.unstack().plot.bar(figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting insight here, very high positives in March, around the beginning of the pandemic. We propose this could be due to encouragement from campus leaders to stay strong when things were beginning to get rough. Also, very low level of negative Tweets in August, the beginning of the semester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the same thing with our datset of Tweets from non admin/uni Tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vader Sentiment Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
